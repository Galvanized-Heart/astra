# Config file for training CpiPredSelfAttnModel

# General settings
seed: 42
project_name: "astra" # wandb project_name
#run_name: "" # Recommend leaving empty for more informative and unique naming for checkpoints and wandb


# Data configuration
data:
  train_path: "data/split/hpo_splits_balanced/hpo_train.csv"
  valid_path: "data/split/hpo_splits_balanced/hpo_valid.csv"
  batch_size: 64
  featurizer_batch_size: 128
  target_columns: ["kcat", "KM", "Ki"] # Set either single value or all 3
  target_transform: "log10"


# Featurizer configuration
featurizers:
  protein:
    name: "ESMFeaturizer"
    params:
      model_name: "facebook/esm2_t33_650M_UR50D"
      max_length: 1022
  ligand:
    name: "MorganFeaturizer"
    params:
      radius: 2
      fp_size: 2048


# Model configuration
model:
  architecture:
    name: "CpiPredSelfAttnModel"
    params: 
      n_heads: 4      # Number of attention heads
      d_k: 64         # Dimension of Key/Query
      d_v: 64         # Dimension of Value
      attn_out_dim: 1 # Output dimension of the attention layer itself
      d_ff: 1024      # Hidden dimension of the final feed-forward network
  
  # Configuration for the Lightning wrapper (AstraModule)
  lightning_module:
    lr: 0.001
    optimizer: "AdamW"
    loss_function:
      name: "MaskedMSELoss"
      params:
        weights: [0.33, 0.33, 0.33] # These will be overwritten by the sweep
    recomposition_func: AdvancedRecomp # Optional
    lr_scheduler: # Optional
      name: "ReduceLROnPlateau"
      params:
        monitor: "valid_loss_epoch"
        mode: "min"
        factor: 0.1
        patience: 3


# Trainer configuration
trainer:
  epochs: 20
  device: "auto" # Options: "cpu", "gpu", "auto"
  callbacks:
    checkpoint:
      monitor: "valid_loss_epoch"
      save_top_k: 1
      mode: "min"